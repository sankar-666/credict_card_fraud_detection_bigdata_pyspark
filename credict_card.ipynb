{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Initiate and Configure Spark**"
      ],
      "metadata": {
        "id": "a4iCW_Cjekaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using ! to execute a command in the command line or terminal\n",
        "# Using pip3 to interact with the Python package manager for Python 3.x\n",
        "# Using install to specify that we want to install a package\n",
        "# Install the PySpark library, which is the Python API for Apache Spark\n",
        "\n",
        "\n",
        "!pip3 install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVzMBHekemfl",
        "outputId": "b0058c14-1a7b-4770-b9b0-53ee74ba3e86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b2704c5806718d28ec97ac682ac0347b603e381b6670c8dba907b7ee283413fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Connect Google Drive**"
      ],
      "metadata": {
        "id": "VYz_OcoMepW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the 'drive' module from the 'google.colab' library to mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting the Google Drive at the '/content/drive' directory\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErO6I3B5erbF",
        "outputId": "a54a7f84-dc49-4a10-9a10-af120c71efa6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the SparkSession class from the pyspark.sql module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a SparkSession named 'spark' to interact with Spark\n",
        "# The 'master' parameter is set to \"local[*]\", which means Spark will run in local mode using all available cores\n",
        "# The 'appName' parameter is set to 'Fraud Detection' to give a name to the Spark application\n",
        "# The 'getOrCreate()' method ensures that if an existing SparkSession is available, it will be reused; otherwise, a new one will be created\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName('Fraud Detection') \\\n",
        "        .getOrCreate()\n",
        "        # .config(\"spark.driver.memory\", \"8g\") \\\n",
        "        # .config(\"spark.kryoserializer.buffer.max\", \"16g\") \\"
      ],
      "metadata": {
        "id": "cOMbMavyetgC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Data Loading and Preprocessing**\n",
        "---"
      ],
      "metadata": {
        "id": "rwSpZCKPevvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify the student who made a contribution and mention their name in the appropriate section of the code.\n",
        "\n",
        "## The students' names who made contributions\n",
        "\n",
        "# load spark_df using spark\n",
        "spark_df = spark.read.csv('/content/drive/MyDrive/big_data/transactions_train.csv',inferSchema=True, header =True)\n",
        "\n",
        "# available columns in this spark_df\n",
        "spark_df.columns"
      ],
      "metadata": {
        "id": "vwVX2WR-ez2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a56673b-18ea-48c8-9d75-1efc281d6ddd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['step',\n",
              " 'type',\n",
              " 'amount',\n",
              " 'nameOrig',\n",
              " 'oldbalanceOrig',\n",
              " 'newbalanceOrig',\n",
              " 'nameDest',\n",
              " 'oldbalanceDest',\n",
              " 'newbalanceDest',\n",
              " 'isFraud']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.describe().show()"
      ],
      "metadata": {
        "id": "FqeW908fe2TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e210277-e1b6-4bc8-c519-a3edb405fe24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+--------+-----------------+-----------+-----------------+------------------+-----------+------------------+------------------+--------------------+\n",
            "|summary|              step|    type|           amount|   nameOrig|   oldbalanceOrig|    newbalanceOrig|   nameDest|    oldbalanceDest|    newbalanceDest|             isFraud|\n",
            "+-------+------------------+--------+-----------------+-----------+-----------------+------------------+-----------+------------------+------------------+--------------------+\n",
            "|  count|           6351193| 6351193|          6351193|    6351193|          6351193|           6351193|    6351193|           6351193|           6351193|             6351193|\n",
            "|   mean|242.55529819358347|    NULL|179815.5359635669|       NULL|834795.6840371998|  856169.582831443|       NULL|1101042.5969942801|1225371.9736932502|0.001215047314732...|\n",
            "| stddev|141.06763627792867|    NULL|603630.9774416926|       NULL|2889959.094210148|2926073.0596211716|       NULL|3398923.5732159945|3674292.5871343063|0.034836348342406516|\n",
            "|    min|                 1| CASH_IN|              0.0|C1000000639|              0.0|               0.0|C1000004082|               0.0|               0.0|                   0|\n",
            "|    max|               699|TRANSFER|    9.244551664E7| C999999784|    5.958504037E7|     4.958504037E7| M999999784|    3.5601588935E8|    3.5617927892E8|                   1|\n",
            "+-------+------------------+--------+-----------------+-----------+-----------------+------------------+-----------+------------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_counts = spark_df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in spark_df.columns])\n",
        "missing_counts.show()\n",
        "\n",
        "# Check for missing values in any row\n",
        "total_missing_count = spark_df.rdd.map(lambda row: sum([1 for x in row if x == None])).sum()\n",
        "print(\"Total missing values in DataFrame: {}\".format(total_missing_count))"
      ],
      "metadata": {
        "id": "0IYnbUMte337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30e65bb-225b-4d0e-fbe1-561cfe61d09d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+\n",
            "|step|type|amount|nameOrig|oldbalanceOrig|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|\n",
            "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+\n",
            "|   0|   0|     0|       0|             0|             0|       0|             0|             0|      0|\n",
            "+----+----+------+--------+--------------+--------------+--------+--------------+--------------+-------+\n",
            "\n",
            "Total missing values in DataFrame: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "# convert column type into numerical value\n",
        "\"\"\"\n",
        "  CASH-IN : 0,\n",
        "  CASH-OUT : 1,\n",
        "  DEBIT: 2,\n",
        "  PAYMENT : 3,\n",
        "  TRANSFER: 4,\n",
        "\n",
        "\"\"\"\n",
        "spark_df=spark_df.withColumn(\"type\",\n",
        "                             when(spark_df.type==\"CASH_IN\", 0)\n",
        "                            .when(spark_df.type==\"CASH_OUT\", 1)\n",
        "                            .when(spark_df.type==\"DEBIT\", 2)\n",
        "                            .when(spark_df.type==\"PAYMENT\", 3)\n",
        "                            .when(spark_df.type==\"TRANSFER\", 4)\n",
        "                            .otherwise(-1))\n",
        "\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "oZ0_rNk-e5kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3890d69c-11ed-4ea6-95a4-ed71ee24973d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+\n",
            "|step|type|   amount|   nameOrig|oldbalanceOrig|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|\n",
            "+----+----+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+\n",
            "|   1|   3|  9839.64|C1231006815|      170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|\n",
            "|   1|   3|  1864.28|C1666544295|       21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|\n",
            "|   1|   4|    181.0|C1305486145|         181.0|           0.0| C553264065|           0.0|           0.0|      1|\n",
            "|   1|   1|    181.0| C840083671|         181.0|           0.0|  C38997010|       21182.0|           0.0|      1|\n",
            "|   1|   3| 11668.14|C2048537720|       41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|\n",
            "|   1|   3|  7817.71|  C90045638|       53860.0|      46042.29| M573487274|           0.0|           0.0|      0|\n",
            "|   1|   3|  7107.77| C154988899|      183195.0|     176087.23| M408069119|           0.0|           0.0|      0|\n",
            "|   1|   3|  7861.64|C1912850431|     176087.23|     168225.59| M633326333|           0.0|           0.0|      0|\n",
            "|   1|   3|  4024.36|C1265012928|        2671.0|           0.0|M1176932104|           0.0|           0.0|      0|\n",
            "|   1|   2|  5337.77| C712410124|       41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|\n",
            "|   1|   2|  9644.94|C1900366749|        4465.0|           0.0| C997608398|       10845.0|     157982.12|      0|\n",
            "|   1|   3|  3099.97| C249177573|       20771.0|      17671.03|M2096539129|           0.0|           0.0|      0|\n",
            "|   1|   3|  2560.74|C1648232591|        5070.0|       2509.26| M972865270|           0.0|           0.0|      0|\n",
            "|   1|   3| 11633.76|C1716932897|       10127.0|           0.0| M801569151|           0.0|           0.0|      0|\n",
            "|   1|   3|  4098.78|C1026483832|      503264.0|     499165.22|M1635378213|           0.0|           0.0|      0|\n",
            "|   1|   1|229133.94| C905080434|       15325.0|           0.0| C476402209|        5083.0|      51513.44|      0|\n",
            "|   1|   3|  1563.82| C761750706|         450.0|           0.0|M1731217984|           0.0|           0.0|      0|\n",
            "|   1|   3|  1157.86|C1237762639|       21156.0|      19998.14|M1877062907|           0.0|           0.0|      0|\n",
            "|   1|   3|   671.64|C2033524545|       15123.0|      14451.36| M473053293|           0.0|           0.0|      0|\n",
            "|   1|   4| 215310.3|C1670993182|         705.0|           0.0|C1100439041|       22425.0|           0.0|      0|\n",
            "+----+----+---------+-----------+--------------+--------------+-----------+--------------+--------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.count()"
      ],
      "metadata": {
        "id": "izS1qfjWe7th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac960287-5dea-40d9-ff83-84c6b7e41731"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6351193"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"nameOrig\", StringType(), True),  # Original column\n",
        "    StructField(\"nameDest\", StringType(), True),  # Original column\n",
        "    StructField(\"nameOrig_hashed\", ArrayType(FloatType()), True),  # Hashed column for nameOrig\n",
        "    StructField(\"nameDest_hashed\", ArrayType(FloatType()), True),  # Hashed column for nameDest\n",
        "])\n",
        "\n",
        "\n",
        "# Define a function to apply the pipeline to each partition\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def hashingFeatures(pdf):\n",
        "    # Define stages for the pipeline within the function\n",
        "    hashingTF = HashingTF(inputCol=\"nameOrig\", outputCol=\"nameOrig_hashed\", numFeatures=10)\n",
        "    hashingTF_dest = HashingTF(inputCol=\"nameDest\", outputCol=\"nameDest_hashed\", numFeatures=10)\n",
        "\n",
        "    # Create the pipeline\n",
        "    pipeline = Pipeline(stages=[hashingTF, hashingTF_dest])\n",
        "\n",
        "    # Fit the pipeline to the data\n",
        "    pipeline_model = pipeline.fit(pdf)\n",
        "\n",
        "    # Transform the data\n",
        "    df_hashed = pipeline_model.transform(pdf)\n",
        "\n",
        "    return df_hashed\n",
        "\n",
        "# Apply hashing to the DataFrame\n",
        "df_hashed = spark_df.groupby(\"nameOrig\").apply(hashingFeatures)\n",
        "\n",
        "df_hashed.show()\n"
      ],
      "metadata": {
        "id": "2dWcGPfye-C0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "c0b686ab-9209-4c4a-dc39-0230479b7bd4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-11-8f1e8861d288>\", line 19, in hashingFeatures\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 139, in wrapper\n    return func(self, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 1677, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.HashingTF\", self.uid)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8f1e8861d288>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mdf_hashed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nameOrig\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashingFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mdf_hashed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-11-8f1e8861d288>\", line 19, in hashingFeatures\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/__init__.py\", line 139, in wrapper\n    return func(self, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/feature.py\", line 1677, in __init__\n    self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.HashingTF\", self.uid)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 80, in _new_java_obj\n    assert sc is not None\nAssertionError\n"
          ]
        }
      ]
    }
  ]
}